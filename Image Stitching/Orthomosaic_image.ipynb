{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to load image: C:\\Users\\hp\\Desktop\\folio 3\\NewFrame (1)\\NewFrame\\panorama\n",
      "Failed to load image: C:\\Users\\hp\\Desktop\\folio 3\\NewFrame (1)\\NewFrame\\ratio test and sift\n",
      "Failed to load image: C:\\Users\\hp\\Desktop\\folio 3\\NewFrame (1)\\NewFrame\\sift and low's ratio\n",
      "Loaded 2 images.\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the images\n",
    "image_dir = r'C:\\Users\\hp\\Desktop\\folio 3\\NewFrame (1)\\NewFrame'\n",
    "\n",
    "# List to store image paths\n",
    "image_paths = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(image_dir):\n",
    "\n",
    "    # Get the path to the image\n",
    "    image_path = os.path.join(image_dir, filename)\n",
    "\n",
    "    # Add it to the list of image paths\n",
    "    image_paths.append(image_path)\n",
    "\n",
    "# Loop through the image paths and load each image\n",
    "images = []\n",
    "\n",
    "# Loop through the image paths and load each image\n",
    "for image_path in image_paths:\n",
    "\n",
    "    # Load image from path and add to list of images\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is not None:\n",
    "        images.append(image)\n",
    "    else:\n",
    "        print(f\"Failed to load image: {image_path}\")\n",
    "\n",
    "# Now you have a list of loaded images in the 'images' variable\n",
    "print(f\"Loaded {len(images)} images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panorama image using ORB and detectAndComoute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load images\n",
    "img1 = cv2.imread(\"C:/Users/hp/Desktop/folio 3/NewFrame (1)/NewFrame/left_cropped_image.jpg\")\n",
    "img2 = cv2.imread(\"C:/Users/hp/Desktop/folio 3/NewFrame (1)/NewFrame/right_cropped_image.jpg\")\n",
    "\n",
    "cv2.namedWindow(\"I2\", cv2.WINDOW_NORMAL)\n",
    "cv2.namedWindow(\"I1\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"I1\", img1)\n",
    "cv2.imshow(\"I2\", img2)\n",
    "\n",
    "# Initialize ORB detectors\n",
    "orb1 = cv2.ORB_create()\n",
    "orb2 = cv2.ORB_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors\n",
    "pts1, desc1 = orb1.detectAndCompute(img1, None)\n",
    "pts2, desc2 = orb2.detectAndCompute(img2, None)\n",
    "\n",
    "# Create a BFMatcher (Brute Force Matcher) with Hamming distance\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# Match descriptors\n",
    "matches = bf.match(desc1, desc2)\n",
    "\n",
    "# Sort matches by distance\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "# Keep only the best matches\n",
    "best_matches = matches[:len(matches) // 2]\n",
    "\n",
    "# Get corresponding points\n",
    "src = [pts1[match.queryIdx].pt for match in best_matches]\n",
    "dst = [pts2[match.trainIdx].pt for match in best_matches]\n",
    "\n",
    "# Find the homography matrix using RANSAC\n",
    "homography, _ = cv2.findHomography(np.array(src), np.array(dst), cv2.RANSAC)\n",
    "\n",
    "# Warp the second image to align with the first image\n",
    "result = cv2.warpPerspective(img2, homography, (img1.shape[1] + img2.shape[1], img1.shape[0]))\n",
    "\n",
    "# Copy the first image onto the result image\n",
    "result[0:img1.shape[0], 0:img1.shape[1]] = img1\n",
    "\n",
    "cv2.namedWindow(\"I3\", cv2.WINDOW_NORMAL)\n",
    "cv2.imshow(\"I3\", result)\n",
    "cv2.imwrite(\"result.jpg\", result)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panorama image using cv2 Sift and BF matcher and LOW'S ratio test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hp\\Desktop\\folio 3\\Orthomosaic_image.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load your input images\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m image1 \u001b[39m=\u001b[39m images[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m image2 \u001b[39m=\u001b[39m images[\u001b[39m1\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Load more images...\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Detect features and compute descriptors\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load your input images\n",
    "image1 = images[0]\n",
    "image2 = images[1]\n",
    "# Load more images...\n",
    "\n",
    "# Detect features and compute descriptors\n",
    "detector = cv2.SIFT_create()\n",
    "kp1, des1 = detector.detectAndCompute(image1, None)\n",
    "kp2, des2 = detector.detectAndCompute(image2, None)\n",
    "# Detect and compute for other images...\n",
    "\n",
    "# Match features between pairs of images\n",
    "matcher = cv2.BFMatcher()\n",
    "matches = matcher.knnMatch(des1, des2, k=2)\n",
    "# Perform matching for other pairs...\n",
    "\n",
    "# Remove false matches using Lowe's ratio test or RANSAC\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "# Filter matches for other pairs...\n",
    "\n",
    "# Calculate homography\n",
    "src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "homography, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "# Calculate homography for other pairs...\n",
    "\n",
    "# Stitch images\n",
    "result = cv2.warpPerspective(image2, homography, (image1.shape[1] + image2.shape[1], image1.shape[0]))\n",
    "result[0:image1.shape[0], 0:image1.shape[1]] = image1\n",
    "\n",
    "# Save or display the final panorama\n",
    "cv2.imwrite('panorama.jpg', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panorama Image Creation    ( works well on the images which have great overlapping region (i.e. two images made from one by cutting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/hp/Desktop/folio 3/NewFrame (1)/NewFrame/Cam05_2023.07.25.13.04.59.991000.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hp\\Desktop\\folio 3\\Orthomosaic_image.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m left_crop_percentage \u001b[39m=\u001b[39m \u001b[39m0.6\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m right_crop_percentage \u001b[39m=\u001b[39m \u001b[39m0.6\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m left_cropped_image, right_cropped_image \u001b[39m=\u001b[39m crop_image(input_image_path, left_crop_percentage, right_crop_percentage)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m left_cropped_image\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mleft_cropped_image.jpg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m right_cropped_image\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mright_cropped_image.jpg\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\hp\\Desktop\\folio 3\\Orthomosaic_image.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcrop_image\u001b[39m(image_path, left_crop_percentage, right_crop_percentage):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     original_image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(image_path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     width, height \u001b[39m=\u001b[39m original_image\u001b[39m.\u001b[39msize\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     left_crop_width \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(width \u001b[39m*\u001b[39m left_crop_percentage)\n",
      "File \u001b[1;32mc:\\Users\\hp\\Desktop\\folio 3\\folio\\Lib\\site-packages\\PIL\\Image.py:3131\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3128\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   3130\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 3131\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   3132\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3134\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/hp/Desktop/folio 3/NewFrame (1)/NewFrame/Cam05_2023.07.25.13.04.59.991000.jpg'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def crop_image(image_path, left_crop_percentage, right_crop_percentage):\n",
    "    original_image = Image.open(image_path)\n",
    "    width, height = original_image.size\n",
    "\n",
    "    left_crop_width = int(width * left_crop_percentage)\n",
    "    right_crop_width = int(width * right_crop_percentage)\n",
    "\n",
    "    left_crop = original_image.crop((0, 0, left_crop_width, height))\n",
    "    right_crop = original_image.crop((width - right_crop_width, 0, width, height))\n",
    "    \n",
    "    return left_crop, right_crop\n",
    "\n",
    "# Example usage\n",
    "input_image_path = 'C:/Users/hp/Desktop/folio 3/NewFrame (1)/NewFrame/Cam05_2023.07.25.13.04.59.991000.jpg'\n",
    "left_crop_percentage = 0.6\n",
    "right_crop_percentage = 0.6\n",
    "\n",
    "left_cropped_image, right_cropped_image = crop_image(input_image_path, left_crop_percentage, right_crop_percentage)\n",
    "left_cropped_image.save('left_cropped_image.jpg')\n",
    "right_cropped_image.save('right_cropped_image.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 images.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "      \n",
    "\n",
    "def ReadImage(ImageFolderPath):\n",
    "        # Directory containing the images\n",
    "    image_dir = r'C:\\Users\\hp\\Desktop\\folio 3\\NewFrame (1)\\NewFrame'\n",
    "\n",
    "    # List to store image paths\n",
    "    image_paths = []\n",
    "\n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(image_dir):\n",
    "\n",
    "        # Get the path to the image\n",
    "        image_path = os.path.join(image_dir, filename)\n",
    "\n",
    "        # Add it to the list of image paths\n",
    "        image_paths.append(image_path)\n",
    "\n",
    "    # Loop through the image paths and load each image\n",
    "    imgs = []\n",
    "\n",
    "    # Loop through the image paths and load each image\n",
    "    for image_path in image_paths:\n",
    "\n",
    "        # Load image from path and add to list of images\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        if image is not None:\n",
    "            imgs.append(image)\n",
    "        else:\n",
    "            print(f\"Failed to load image: {image_path}\")\n",
    "\n",
    "    # Now you have a list of loaded images in the 'images' variable\n",
    "    print(f\"Loaded {len(imgs)} images.\")\n",
    "\n",
    "    return imgs\n",
    "\n",
    "\n",
    "    \n",
    "def FindMatches(BaseImage, SecImage):\n",
    "    # Using SIFT to find the keypoints and decriptors in the images\n",
    "    Sift = cv2.SIFT_create()\n",
    "    BaseImage_kp, BaseImage_des = Sift.detectAndCompute(cv2.cvtColor(BaseImage, cv2.COLOR_BGR2GRAY), None)\n",
    "    SecImage_kp, SecImage_des = Sift.detectAndCompute(cv2.cvtColor(SecImage, cv2.COLOR_BGR2GRAY), None)\n",
    "\n",
    "    # Using Brute Force matcher to find matches.\n",
    "    BF_Matcher = cv2.BFMatcher()\n",
    "    InitialMatches = BF_Matcher.knnMatch(BaseImage_des, SecImage_des, k=2)\n",
    "\n",
    "    # Applytng ratio test and filtering out the good matches.\n",
    "    GoodMatches = []\n",
    "    for m, n in InitialMatches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            GoodMatches.append([m])\n",
    "\n",
    "    return GoodMatches, BaseImage_kp, SecImage_kp\n",
    "\n",
    "\n",
    "\n",
    "def FindHomography(Matches, BaseImage_kp, SecImage_kp):\n",
    "    # If less than 4 matches found, exit the code.\n",
    "    if len(Matches) < 4:\n",
    "        print(\"\\nNot enough matches found between the images.\\n\")\n",
    "        exit(0)\n",
    "\n",
    "    # Storing coordinates of points corresponding to the matches found in both the images\n",
    "    BaseImage_pts = []\n",
    "    SecImage_pts = []\n",
    "    for Match in Matches:\n",
    "        BaseImage_pts.append(BaseImage_kp[Match[0].queryIdx].pt)\n",
    "        SecImage_pts.append(SecImage_kp[Match[0].trainIdx].pt)\n",
    "\n",
    "    # Changing the datatype to \"float32\" for finding homography\n",
    "    BaseImage_pts = np.float32(BaseImage_pts)\n",
    "    SecImage_pts = np.float32(SecImage_pts)\n",
    "\n",
    "    # Finding the homography matrix(transformation matrix).\n",
    "    (HomographyMatrix, Status) = cv2.findHomography(SecImage_pts, BaseImage_pts, cv2.RANSAC, 4.0)\n",
    "\n",
    "    return HomographyMatrix, Status\n",
    "\n",
    "    \n",
    "def GetNewFrameSizeAndMatrix(HomographyMatrix, Sec_ImageShape, Base_ImageShape):\n",
    "    # Reading the size of the image\n",
    "    (Height, Width) = Sec_ImageShape\n",
    "    \n",
    "    # Taking the matrix of initial coordinates of the corners of the secondary image\n",
    "    # Stored in the following format: [[x1, x2, x3, x4], [y1, y2, y3, y4], [1, 1, 1, 1]]\n",
    "    # Where (xt, yt) is the coordinate of the i th corner of the image. \n",
    "    InitialMatrix = np.array([[0, Width - 1, Width - 1, 0],\n",
    "                              [0, 0, Height - 1, Height - 1],\n",
    "                              [1, 1, 1, 1]])\n",
    "    \n",
    "    # Finding the final coordinates of the corners of the image after transformation.\n",
    "    # NOTE: Here, the coordinates of the corners of the frame may go out of the \n",
    "    # frame(negative values). We will correct this afterwards by updating the \n",
    "    # homography matrix accordingly.\n",
    "    FinalMatrix = np.dot(HomographyMatrix, InitialMatrix)\n",
    "\n",
    "    [x, y, c] = FinalMatrix\n",
    "    x = np.divide(x, c)\n",
    "    y = np.divide(y, c)\n",
    "\n",
    "    # Finding the dimentions of the stitched image frame and the \"Correction\" factor\n",
    "    min_x, max_x = int(round(min(x))), int(round(max(x)))\n",
    "    min_y, max_y = int(round(min(y))), int(round(max(y)))\n",
    "\n",
    "    New_Width = max_x\n",
    "    New_Height = max_y\n",
    "    Correction = [0, 0]\n",
    "    if min_x < 0:\n",
    "        New_Width -= min_x\n",
    "        Correction[0] = abs(min_x)\n",
    "    if min_y < 0:\n",
    "        New_Height -= min_y\n",
    "        Correction[1] = abs(min_y)\n",
    "    \n",
    "    # Again correcting New_Width and New_Height\n",
    "    # Helpful when secondary image is overlaped on the left hand side of the Base image.\n",
    "    if New_Width < Base_ImageShape[1] + Correction[0]:\n",
    "        New_Width = Base_ImageShape[1] + Correction[0]\n",
    "    if New_Height < Base_ImageShape[0] + Correction[1]:\n",
    "        New_Height = Base_ImageShape[0] + Correction[1]\n",
    "\n",
    "    # Finding the coordinates of the corners of the image if they all were within the frame.\n",
    "    x = np.add(x, Correction[0])\n",
    "    y = np.add(y, Correction[1])\n",
    "    OldInitialPoints = np.float32([[0, 0],\n",
    "                                   [Width - 1, 0],\n",
    "                                   [Width - 1, Height - 1],\n",
    "                                   [0, Height - 1]])\n",
    "    NewFinalPonts = np.float32(np.array([x, y]).transpose())\n",
    "\n",
    "    # Updating the homography matrix. Done so that now the secondary image completely \n",
    "    # lies inside the frame\n",
    "    HomographyMatrix = cv2.getPerspectiveTransform(OldInitialPoints, NewFinalPonts)\n",
    "    \n",
    "    return [New_Height, New_Width], Correction, HomographyMatrix\n",
    "\n",
    "\n",
    "\n",
    "def StitchImages(BaseImage, SecImage):\n",
    "    # Applying Cylindrical projection on SecImage\n",
    "    SecImage_Cyl, mask_x, mask_y = ProjectOntoCylinder(SecImage)\n",
    "\n",
    "    # Getting SecImage Mask\n",
    "    SecImage_Mask = np.zeros(SecImage_Cyl.shape, dtype=np.uint8)\n",
    "    SecImage_Mask[mask_y-1, mask_x-1, :] = 255\n",
    "\n",
    "    # Finding matches between the 2 images and their keypoints\n",
    "    Matches, BaseImage_kp, SecImage_kp = FindMatches(BaseImage, SecImage_Cyl)\n",
    "    \n",
    "    # Finding homography matrix.\n",
    "    HomographyMatrix, Status = FindHomography(Matches, BaseImage_kp, SecImage_kp)\n",
    "    \n",
    "    # Finding size of new frame of stitched images and updating the homography matrix \n",
    "    NewFrameSize, Correction, HomographyMatrix = GetNewFrameSizeAndMatrix(HomographyMatrix, SecImage_Cyl.shape[:2], BaseImage.shape[:2])\n",
    "\n",
    "    # Finally placing the images upon one another.\n",
    "    SecImage_Transformed = cv2.warpPerspective(SecImage_Cyl, HomographyMatrix, (NewFrameSize[1], NewFrameSize[0]))\n",
    "    SecImage_Transformed_Mask = cv2.warpPerspective(SecImage_Mask, HomographyMatrix, (NewFrameSize[1], NewFrameSize[0]))\n",
    "    BaseImage_Transformed = np.zeros((NewFrameSize[0], NewFrameSize[1], 3), dtype=np.uint8)\n",
    "    BaseImage_Transformed[Correction[1]:Correction[1]+BaseImage.shape[0], Correction[0]:Correction[0]+BaseImage.shape[1]] = BaseImage\n",
    "\n",
    "    StitchedImage = cv2.bitwise_or(SecImage_Transformed, cv2.bitwise_and(BaseImage_Transformed, cv2.bitwise_not(SecImage_Transformed_Mask)))\n",
    "\n",
    "    return StitchedImage\n",
    "\n",
    "\n",
    "def Convert_xy(x, y):\n",
    "    global center, f\n",
    "\n",
    "    xt = ( f * np.tan( (x - center[0]) / f ) ) + center[0]\n",
    "    yt = ( (y - center[1]) / np.cos( (x - center[0]) / f ) ) + center[1]\n",
    "    \n",
    "    return xt, yt\n",
    "\n",
    "\n",
    "def ProjectOntoCylinder(InitialImage):\n",
    "    global w, h, center, f\n",
    "    h, w = InitialImage.shape[:2]\n",
    "    center = [w // 2, h // 2]\n",
    "    f = 1100       # 1100 field; 1000 Sun; 1500 Rainier; 1050 Helens\n",
    "    \n",
    "    # Creating a blank transformed image\n",
    "    TransformedImage = np.zeros(InitialImage.shape, dtype=np.uint8)\n",
    "    \n",
    "    # Storing all coordinates of the transformed image in 2 arrays (x and y coordinates)\n",
    "    AllCoordinates_of_ti =  np.array([np.array([i, j]) for i in range(w) for j in range(h)])\n",
    "    ti_x = AllCoordinates_of_ti[:, 0]\n",
    "    ti_y = AllCoordinates_of_ti[:, 1]\n",
    "    \n",
    "    # Finding corresponding coordinates of the transformed image in the initial image\n",
    "    ii_x, ii_y = Convert_xy(ti_x, ti_y)\n",
    "\n",
    "    # Rounding off the coordinate values to get exact pixel values (top-left corner)\n",
    "    ii_tl_x = ii_x.astype(int)\n",
    "    ii_tl_y = ii_y.astype(int)\n",
    "\n",
    "    # Finding transformed image points whose corresponding \n",
    "    # initial image points lies inside the initial image\n",
    "    GoodIndices = (ii_tl_x >= 0) * (ii_tl_x <= (w-2)) * \\\n",
    "                  (ii_tl_y >= 0) * (ii_tl_y <= (h-2))\n",
    "\n",
    "    # Removing all the outside points from everywhere\n",
    "    ti_x = ti_x[GoodIndices]\n",
    "    ti_y = ti_y[GoodIndices]\n",
    "    \n",
    "    ii_x = ii_x[GoodIndices]\n",
    "    ii_y = ii_y[GoodIndices]\n",
    "\n",
    "    ii_tl_x = ii_tl_x[GoodIndices]\n",
    "    ii_tl_y = ii_tl_y[GoodIndices]\n",
    "\n",
    "    # Bilinear interpolation\n",
    "    dx = ii_x - ii_tl_x\n",
    "    dy = ii_y - ii_tl_y\n",
    "\n",
    "    weight_tl = (1.0 - dx) * (1.0 - dy)\n",
    "    weight_tr = (dx)       * (1.0 - dy)\n",
    "    weight_bl = (1.0 - dx) * (dy)\n",
    "    weight_br = (dx)       * (dy)\n",
    "    \n",
    "    TransformedImage[ti_y, ti_x, :] = ( weight_tl[:, None] * InitialImage[ii_tl_y,     ii_tl_x,     :] ) + \\\n",
    "                                      ( weight_tr[:, None] * InitialImage[ii_tl_y,     ii_tl_x + 1, :] ) + \\\n",
    "                                      ( weight_bl[:, None] * InitialImage[ii_tl_y + 1, ii_tl_x,     :] ) + \\\n",
    "                                      ( weight_br[:, None] * InitialImage[ii_tl_y + 1, ii_tl_x + 1, :] )\n",
    "\n",
    "\n",
    "    # Getting x coorinate to remove black region from right and left in the transformed image\n",
    "    min_x = min(ti_x)\n",
    "\n",
    "    # Cropping out the black region from both sides (using symmetricity)\n",
    "    TransformedImage = TransformedImage[:, min_x : -min_x, :]\n",
    "\n",
    "    return TransformedImage, ti_x-min_x, ti_y\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Reading images.\n",
    "    Images = ReadImage(\"C:/Users/hp/Desktop/folio 3/NewFrame (1)/NewFrame\")\n",
    "    \n",
    "    BaseImage, _, _ = ProjectOntoCylinder(Images[0])\n",
    "    for i in range(1, len(Images)):\n",
    "        \n",
    "        StitchedImage = StitchImages(BaseImage, Images[i])\n",
    "\n",
    "        BaseImage = StitchedImage.copy()    \n",
    "\n",
    "    cv2.imwrite(\"Stitched_Panorama.png\", BaseImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratio Test and Sift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the images you want to stitch\n",
    "image1 = cv2.imread('C:/Users/hp/Desktop/folio 3/NewFrame (1)/NewFrame/panorama/left_cropped_image.jpg')\n",
    "image2 = cv2.imread('C:/Users/hp/Desktop/folio 3/NewFrame (1)/NewFrame/panorama/right_cropped_image.jpg')\n",
    "\n",
    "# Convert the images to grayscale (optional, but can help with feature detection)\n",
    "gray_image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray_image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Initialize the SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Find the keypoints and descriptors for the images\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(gray_image1, None)\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(gray_image2, None)\n",
    "\n",
    "# Initialize a FLANN matcher\n",
    "flann = cv2.FlannBasedMatcher(dict(algorithm=1, trees=5), {})\n",
    "\n",
    "# Find matches between the descriptors of the two images\n",
    "matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "# Ratio test to filter good matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# If we have enough good matches, we can proceed with image stitching\n",
    "if len(good_matches) > 10:\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Find the transformation matrix using RANSAC\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "    # Warp the first image to the second image's perspective\n",
    "    stitched_image = cv2.warpPerspective(image1, M, (image1.shape[1] + image2.shape[1], image1.shape[0]))\n",
    "\n",
    "    # Copy the second image to the stitched image\n",
    "    stitched_image[0:image2.shape[0], 0:image2.shape[1]] = image2\n",
    "\n",
    "    # Display the stitched image\n",
    "    cv2.imshow('Stitched Image', stitched_image)\n",
    "\n",
    "    cv2.imwrite('stitched_image_hello.jpg', stitched_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(\"Not enough good matches to stitch the images.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthomosaic image creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a feature detection object\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Find key points and descriptors for each image\n",
    "keypoints_and_descriptors = [orb.detectAndCompute(image, None) for image in images]\n",
    "\n",
    "# Match features between image pairs\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(keypoints_and_descriptors[0][1], keypoints_and_descriptors[1][1])\n",
    "\n",
    "# Extract matched key points\n",
    "src_pts = np.float32([keypoints_and_descriptors[0][0][m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([keypoints_and_descriptors[1][0][m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Estimate homography using RANSAC\n",
    "homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Warp the second image to align with the first\n",
    "result = cv2.warpPerspective(images[1], homography, (images[0].shape[1], images[0].shape[0]))\n",
    "\n",
    "# Stitch the aligned images together\n",
    "stitched_image = cv2.addWeighted(images[0], 0.5, result, 0.5, 0)\n",
    "\n",
    "cv2.imshow('Stitched Image', stitched_image)\n",
    "\n",
    "#save the stitched image\n",
    "\n",
    "cv2.imwrite(\"stitched_image.jpg\", stitched_image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 29-8-2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Create a feature detection object\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "\n",
    "feature_detector = cv2.SIFT_create()\n",
    "\n",
    "# Find key points and descriptors for each image\n",
    "keypoints_and_descriptors = [orb.detectAndCompute(image, None) for image in images]\n",
    "\n",
    "# Create a FLANN based matcher\n",
    "FLANN_INDEX_LSH = 6\n",
    "index_params = dict(algorithm=FLANN_INDEX_LSH, table_number=6, key_size=12, multi_probe_level=1)\n",
    "search_params = dict(checks=100)  # We can adjust the number of checks\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# Match features between image pairs\n",
    "matches = flann.knnMatch(keypoints_and_descriptors[0][1], keypoints_and_descriptors[1][1], k=2)\n",
    "\n",
    "# Apply Lowe's ratio test to filter good matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:   # we can adjust the ratio here too\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract matched key points\n",
    "src_pts = np.float32([keypoints_and_descriptors[0][0][m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([keypoints_and_descriptors[1][0][m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Estimate homography using RANSAC\n",
    "homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Warp the second image to align with the first\n",
    "h, w = images[0].shape[:2]\n",
    "aligned_image = cv2.warpPerspective(images[1], homography, (w, h))\n",
    "\n",
    "# Create a mask to blend the images smoothly\n",
    "mask = np.zeros_like(images[0], dtype=np.uint8)\n",
    "cv2.fillConvexPoly(mask, np.int32(dst_pts), (255, 255, 255), cv2.LINE_AA)\n",
    "\n",
    "# Blend the images\n",
    "stitched_image = cv2.seamlessClone(aligned_image, images[0], mask, (w // 2, h // 2), cv2.NORMAL_CLONE)\n",
    "\n",
    "# Save the stitched image\n",
    "cv2.imwrite(\"stitched_image.jpg\", stitched_image)\n",
    "\n",
    "# Display the stitched image\n",
    "cv2.imshow('Stitched Image', stitched_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a feature detection object\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Find key points and descriptors for each image\n",
    "keypoints_and_descriptors = [orb.detectAndCompute(image, None) for image in images]\n",
    "\n",
    "# Create a FLANN based matcher\n",
    "FLANN_INDEX_LSH = 6\n",
    "index_params = dict(algorithm=FLANN_INDEX_LSH, table_number=6, key_size=12, multi_probe_level=1)\n",
    "search_params = dict(checks=50)  # You can adjust the number of checks\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "# Match features between image pairs\n",
    "matches = flann.knnMatch(keypoints_and_descriptors[0][1], keypoints_and_descriptors[1][1], k=2)\n",
    "\n",
    "# Apply Lowe's ratio test to filter good matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.7 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract matched key points\n",
    "src_pts = np.float32([keypoints_and_descriptors[0][0][m.queryIdx].pt for m in good_matches])\n",
    "dst_pts = np.float32([keypoints_and_descriptors[1][0][m.trainIdx].pt for m in good_matches])\n",
    "\n",
    "# Estimate homography using RANSAC\n",
    "homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Warp the second image to align with the first\n",
    "h, w = images[0].shape[:2]\n",
    "aligned_image = cv2.warpPerspective(images[1], homography, (w, h))\n",
    "\n",
    "\n",
    "\n",
    "# Create a mask to blend the images smoothly\n",
    "mask = np.zeros_like(images[0], dtype=np.uint8)\n",
    "cv2.fillConvexPoly(mask, np.int32(dst_pts), (255, 255, 255), cv2.LINE_AA)\n",
    "\n",
    "# Convert the mask to grayscale (single-channel)\n",
    "mask_gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Feather blending\n",
    "feather_amount = 50  # You can adjust the feather amount\n",
    "feather_mask = cv2.distanceTransform(255 - mask_gray, cv2.DIST_L2, cv2.DIST_MASK_PRECISE)\n",
    "feather = cv2.GaussianBlur(feather_mask, (2 * feather_amount + 1, 2 * feather_amount + 1), 0)\n",
    "feather = np.clip(feather, 0, 255)\n",
    "\n",
    "stitched_image = np.zeros_like(images[0])\n",
    "for c in range(3):\n",
    "    stitched_image[:, :, c] = (images[0][:, :, c] * feather + aligned_image[:, :, c] * (255 - feather)) // 255\n",
    "\n",
    "# Save the stitched image\n",
    "cv2.imwrite(\"stitched_image.jpg\", stitched_image)\n",
    "\n",
    "# Display the stitched image\n",
    "cv2.imshow('Stitched Image', stitched_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m cv2\u001b[39m.\u001b[39mimwrite(output_filename, orthomosaic)\n\u001b[0;32m     70\u001b[0m \u001b[39m# Visualize the orthomosaic\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[0;32m     72\u001b[0m plt\u001b[39m.\u001b[39mimshow(cv2\u001b[39m.\u001b[39mcvtColor(orthomosaic, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB))\n\u001b[0;32m     73\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Feature Extraction\n",
    "\n",
    "feature_detector = cv2.SIFT_create()\n",
    "\n",
    "# Extract keypoints and descriptors for each image\n",
    "keypoints = []\n",
    "descriptors = []\n",
    "for img in images:\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    kp, desc = feature_detector.detectAndCompute(gray, None)\n",
    "    keypoints.append(kp)\n",
    "    descriptors.append(desc)\n",
    "\n",
    "# Step 3: Image Registration\n",
    "# Select a reference image and match keypoints with descriptors\n",
    "reference_image_index = 0\n",
    "reference_keypoints = keypoints[reference_image_index]\n",
    "reference_descriptors = descriptors[reference_image_index]\n",
    "\n",
    "# Initialize an empty list to store transformation matrices\n",
    "transformations = []\n",
    "\n",
    "# Match keypoints and compute transformation matrices for each image\n",
    "for i in range(len(images)):\n",
    "    if i == reference_image_index:\n",
    "        transformations.append(np.eye(3))  # Identity matrix for reference image\n",
    "        continue\n",
    "\n",
    "    matcher = cv2.BFMatcher()\n",
    "    matches = matcher.knnMatch(reference_descriptors, descriptors[i], k=2)\n",
    "\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    src_pts = np.float32([reference_keypoints[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints[i][m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Estimate the transformation using RANSAC\n",
    "    M, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "    transformations.append(M)\n",
    "\n",
    "# Step 4: Image Transformation\n",
    "# Calculate the size of the output canvas\n",
    "\n",
    "max_width = max([img.shape[1] for img in images])\n",
    "max_height = max([img.shape[0] for img in images])\n",
    "output_shape = (max_height, max_width)\n",
    "\n",
    "# Transform each image and create the orthomosaic\n",
    "orthomosaic = np.zeros((output_shape[0], output_shape[1], 3), dtype=np.uint8)\n",
    "for i in range(len(images)):\n",
    "    img = images[i]\n",
    "    M = transformations[i]\n",
    "\n",
    "    # Warp the image to align with the reference image\n",
    "    warped = cv2.warpPerspective(img, M, output_shape[::-1])\n",
    "\n",
    "    # Add the warped image to the orthomosaic\n",
    "    orthomosaic = cv2.add(orthomosaic, warped)\n",
    "\n",
    "output_filename = \"orthomosaic.jpg\"\n",
    "cv2.imwrite(output_filename, orthomosaic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/hp/Desktop/folio 3/NewFrame (1)/NewFrame/0.jpg\n",
      "C:/Users/hp/Desktop/folio 3/NewFrame (1)/NewFrame/1.jpg\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.image' has no attribute 'sift'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hp\\Desktop\\folio 3\\Orthomosaic_image.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m images \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mresize(image, [\u001b[39m640\u001b[39m, \u001b[39m480\u001b[39m]) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#images = [tf.image.grayscale(image) for image in images]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#images = [tf.image.normalize(image) for image in images]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Extract SIFT features\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m images_sift \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49msift(image) \u001b[39mfor\u001b[39;49;00m image \u001b[39min\u001b[39;49;00m images]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Compute the essential matrix\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m essential_matrix \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mcompute_essential_matrix(images_sift[\u001b[39m0\u001b[39m], images_sift[\u001b[39m1\u001b[39m])\n",
      "\u001b[1;32mc:\\Users\\hp\\Desktop\\folio 3\\Orthomosaic_image.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m images \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mresize(image, [\u001b[39m640\u001b[39m, \u001b[39m480\u001b[39m]) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#images = [tf.image.grayscale(image) for image in images]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#images = [tf.image.normalize(image) for image in images]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Extract SIFT features\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m images_sift \u001b[39m=\u001b[39m [tf\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49msift(image) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Compute the essential matrix\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hp/Desktop/folio%203/Orthomosaic_image.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m essential_matrix \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mcompute_essential_matrix(images_sift[\u001b[39m0\u001b[39m], images_sift[\u001b[39m1\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.image' has no attribute 'sift'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the images\n",
    "images = []\n",
    "for i in range(2):\n",
    "    image_path = 'C:/Users/hp/Desktop/folio 3/NewFrame (1)/NewFrame/{}.jpg'.format(i)\n",
    "    print(image_path)\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    images.append(image)\n",
    "\n",
    "# Preprocess the images\n",
    "images = [tf.image.resize(image, [640, 480]) for image in images]\n",
    "#images = [tf.image.grayscale(image) for image in images]\n",
    "#images = [tf.image.normalize(image) for image in images]\n",
    "\n",
    "# Extract SIFT features\n",
    "images_sift = [tf.image.sift(image) for image in images]\n",
    "\n",
    "# Compute the essential matrix\n",
    "essential_matrix = tf.image.compute_essential_matrix(images_sift[0], images_sift[1])\n",
    "\n",
    "# Perform image registration\n",
    "aligned_images = [tf.image.register(image, images_sift[0], essential_matrix) for image in images]\n",
    "\n",
    "# Stitch the images together\n",
    "stitched_image = tf.image.concat(aligned_images, axis=0)\n",
    "\n",
    "# Save the stitched image\n",
    "tf.io.write_file(stitched_image, 'path/to/stitched_image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "folio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
